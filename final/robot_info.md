# Key Files

## steering_NN.py
The steering_NN node imports a trained Dave2v3 model's weights & biases from a .pt file. A .pt file contains the weights and biases of a trained model in the form of a state_dict. This still requires the underlying model to be within your workspace. The node uses the imported NN to steer the robot by passing in as input an unsplit image in real time from the `/image_raw` topic. The model inferences are being computed locally on the robot. The node also uses LIDAR data to stop the robot when it is too close to an obstacle, for this the LIDAR needs to be subscribed to the `/scan` topic which is published by the sllidar_a3_launch.py file. Before this file can be run the usb_cam node and the sllidar_a3 node need to be run. Please refer to the rosbot_setup.md file in the /docs folder on how to run these nodes.


## drive.py
Our drive node was developed in order to take input from the XBOX One Controller and publish velocities to the ROSbot. The node is subscribed to the `/joy` topic which contains published data from the XBOX One Controller (button presses and axes movements). The Joy package is a ROS2 driver that is able to parse input from various controllers and output them in a standardized ROS2 message. This data is then parsed and interpreted as velocites for the ROSbot (the left joystick controls the linear velocity of the ROSbot and the right joystick controls the angular velocity of the ROSbot). The velocities are published on the `/cmd_vel` topic which is interpreted by the ROSbot as velocity commands.

## data_collection_launch.py
This is our launch file that was created to quickly start the joy, drive, and camera nodes. This launch file (or the three nodes it contains) must be laucnhed prior to running the `ros2_data_collection` node. The file and instructions on how to run the file can be found in the launch folder in the base directory of this repository.

## ros2_data_collection.py
The ros2_data_collection node is run following the launch of the data_collection_launch.py file. This file is subscribed to the following topics: `/joy`, `/cmd_vel`, `/image_raw`. The output of this node is a CSV file containing data used for model training as well as images captured by the ROSbot's ZED2 camera at the specified rate. The CSV file contains a column for the image name (this is used later in training to associate an image to the angular velocity at that moment), a column for the linear speed (gathered from the `/cmd_vel` topic), a column for the angular speed (gathered from the `/cmd_vel` topic), and a column for whether or not the robot is in a "hallway turn" (gathered from the `/joy` topic - when a button was pressed and held down on our XBOX One Controller, we noted that as a "hallway turn"). This is used later in data augmentation to create more training instances where the robot is turning. All the images are captured from the `/image_raw` topic and are not split into a left and right image.


Our data collection evolved a lot throughout the process. We first started off by commanding velocity entirely based off the joystick input. But because we were only predicting turn velocity, we decided that keeping linear acceleration constant would be best to gather the best training data. After training multiple models with this data collection method, we still saw poor wall avoidance and turn detection quality. For example, our robot would approach a left turn and would turn sharp left before actually being in a safe distance to make the turn, or run directly into walls. We then wanted to decrease the number of confounding variables as low as possible so we started thresholding the turn velocity ``(ex. if joy input <= .5, turn velocity = 0.3)``. This was also necessary for how we decided to improve our data as discussed in the augmentation section. We basically realized that turning towards the wall was behavior that we didn't want our model to learn, so we needed a way to figure out when we were turning towards the wall during data collect. This was done post-hoc, and was only possible because we had our thresholding. We also collect information regarding if the robot is making a "hallway turn", rather than an S turn, through the press of a bumper on the Xbox controller. Overall, our data collection process was very involved for the driver and required a lot of attention during the process.

For driving an S: The driver does a "soft" turn towards the wall so we can later negate the turn velocity. This is done by lightly pushing the joystick towards the direction of the wall, and will help the neural network learn to drive away from a wall. Then when close to the wall, we do a "hard" turn, by pushing the joystick hard in the direction of the away from the wall. This value won't be negated, because it is helping the neural network learn to drive away from the wall. 

For taking a "hallway turn": The driver presses and holds the right bumper on the Xbox controller and can move in any direction of their choosing. Turns have to be executed extremely carefully, because they will be later augmented many times to help the network learn this behavior more consistently. So an error making a turn may have a detrimental impact to the model. We also held this turn toggle well in advance to the turn so we could reinforce the correct behaviors the network should take before a turn. 


